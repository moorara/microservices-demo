#
# Ref:
#   https://docs.fluentd.org
#   https://docs.fluentd.org/quickstart/life-of-a-fluentd-event
#   https://docs.fluentd.org/configuration/config-file
#


# https://docs.fluentd.org/input/forward
# Listen to a TCP socket to receive the event stream. It also listens to an UDP socket.
<source>
  @type forward
  bind 0.0.0.0
  port 24224
</source>

# https://docs.fluentd.org/deployment/logging
<system>
  log_level debug
  <log>
    format json
  </log>
</system>

# https://docs.fluentd.org/filter/grep
# Filter logs in json and reject non-json logs
<filter docker.**>
  @type grep
  <regexp>
    key log
    pattern ^{.*}$
  </regexp>
</filter>

# https://docs.fluentd.org/filter/parser
# https://docs.fluentd.org/parser/json
# Parse logs as json and add them to event record
# "reserve_time" keeps the fluentd event time
<filter docker.**>
  @type parser
  key_name log
  reserve_time true
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# https://docs.fluentd.org/filter/record_transformer
# Mutate Traefik logs in order to make them consistent with other logs
# Traefik logs "time" in RFC3339 which fluentd parser cannot parse it.
# So, we need to make sure Traefik logs include "timestamp" with same format as other logs.
<filter docker.traefik.**>
  @type record_transformer
  enable_ruby true
  remove_keys msg
  <record>
    message ${record["msg"]}
    timestamp ${time.strftime("%Y-%m-%dT%H:%M:%S%z")}
  </record>
</filter>

# https://docs.fluentd.org/filter/record_transformer
# Remove keys from event record and construct new keys
<filter docker.**>
  @type record_transformer
  enable_ruby true
  remove_keys log
  <record>
    environment local
    region local
    container_image ${tag_parts[2,10].join(".")}
  </record>
</filter>

# https://docs.fluentd.org/output/copy
# https://docs.fluentd.org/output/stdout
# https://docs.fluentd.org/output/elasticsearch
# https://docs.fluentd.org/buffer
<match docker.**>
  @type copy
  <store>
    @type stdout
  </store>
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    # user fluent
    # password secret
    index_name fluentd
    type_name fluentd
    include_tag_key true
    include_timestamp true
    reconnect_on_error true
    <buffer>
      @type memory
      flush_interval 5s
    </buffer>
  </store>
</match>
